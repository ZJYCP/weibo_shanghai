{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting wordcloud\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/d6/af/55c7f2aa0997147943b474a74bab8deb17e7cf935b9abb8798d724c57721/wordcloud-1.6.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting matplotlib (from wordcloud)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/61/42/3e92d7aa64295483fbca20a86c89b34d0cb43cffaadaffe028793902d790/matplotlib-3.1.2-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1MB 33kB/s eta 0:00:01�█████▎  | 12.0MB 33kB/s eta 0:00:34\n",
      "\u001b[?25hCollecting pillow (from wordcloud)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.mirrors.ustc.edu.cn', port=443): Read timed out. (read timeout=15)\")': /simple/pillow/\u001b[0m\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/89/3e/31c2e5385d7588016c6f7ac552e81c3fff2bef4bc61b6f82f8177752405c/Pillow-6.2.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.6.1 (from wordcloud)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/9b/af/4fc72f9d38e43b092e91e5b8cb9956d25b2e3ff8c75aed95df5569e4734e/numpy-1.17.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->wordcloud)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/93/f8/518fb0bb89860eea6ff1b96483fbd9236d5ee991485d0f3eceff1770f654/kiwisolver-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (90kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 36.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib->wordcloud)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='mirrors.tuna.tsinghua.edu.cn', port=443): Read timed out. (read timeout=15)\")': /pypi/web/simple/cycler/\u001b[0m\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib->wordcloud)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 9.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib->wordcloud)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c0/0c/fc2e007d9a992d997f04a80125b0f183da7fb554f1de701bbb70a8e7d479/pyparsing-2.4.5-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 33.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from kiwisolver>=1.0.1->matplotlib->wordcloud)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/54/28/c45d8b54c1339f9644b87663945e54a8503cfef59cf0f65b3ff5dd17cf64/setuptools-42.0.2-py2.py3-none-any.whl (583kB)\n",
      "\u001b[K     |████████████████████████████████| 583kB 57.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six (from cycler>=0.10->matplotlib->wordcloud)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Installing collected packages: setuptools, kiwisolver, six, cycler, python-dateutil, numpy, pyparsing, matplotlib, pillow, wordcloud\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.2 numpy-1.17.4 pillow-6.2.1 pyparsing-2.4.5 python-dateutil-2.8.1 setuptools-42.0.2 six-1.13.0 wordcloud-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install jieba -t /home/aistudio/external-libraries\n",
    "!pip install pymysql -t /home/aistudio/external-libraries\n",
    "!pip install wordcloud -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "!pip install paddlehub==1.1.1\n",
    "!pip install pyahocorasick\n",
    "import paddlehub as hub\n",
    "\n",
    "import paddle.fluid as fluid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "module = hub.Module(name=\"senta_bilstm\")\n",
    "dataset = hub.dataset.ChnSentiCorp()\n",
    "# 生成Reader\n",
    "reader = hub.reader.LACClassifyReader(\n",
    "    dataset=dataset, vocab_path=module.get_vocab_path())\n",
    "# 选择Fine-Tune优化策略\n",
    "strategy = hub.AdamWeightDecayStrategy(\n",
    "    weight_decay=0.01,\n",
    "    warmup_proportion=0.1,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler=\"linear_decay\",\n",
    "    optimizer_name=\"adam\")\n",
    "# 选择运行时配置\n",
    "config = hub.RunConfig(\n",
    "    use_data_parallel=True,\n",
    "    use_pyreader=False,\n",
    "    use_cuda=True,\n",
    "    num_epoch=1,\n",
    "    checkpoint_dir=\"nlp_senta_turtorial_demo\",\n",
    "    batch_size=32,\n",
    "    log_interval=10,\n",
    "    eval_interval=50,\n",
    "    strategy=strategy)\n",
    "# 组建Finetune Task\n",
    "inputs, outputs, program = module.context(trainable=True)\n",
    "\n",
    "sent_feature = outputs[\"sentence_feature\"]\n",
    "\n",
    "feed_list = [inputs[\"words\"].name]\n",
    "\n",
    "cls_task = hub.TextClassifierTask(\n",
    "    data_reader=reader,\n",
    "    feature=sent_feature,\n",
    "    feed_list=feed_list,\n",
    "    num_classes=dataset.num_labels,\n",
    "    config=config)\n",
    "# 模型训练\n",
    "cls_task.finetune_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-06 01:53:56,023] [    INFO] - Installing senta_bilstm module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading senta_bilstm\n",
      "[==================================================] 100.00%\n",
      "Uncompress /home/aistudio/.paddlehub/cache/senta_bilstm\n",
      "[==================================================] 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-06 01:54:17,181] [    INFO] - Successfully installed senta_bilstm-1.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading lac\n",
      "[==================================================] 100.00%\n",
      "Uncompress /home/aistudio/.paddlehub/cache/lac\n",
      "[==================================================] 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-06 01:54:20,594] [    INFO] - 13 pretrained paramaters loaded by PaddleHub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading chnsenticorp.tar.gz\n",
      "[==================================================] 100.00%\n",
      "Uncompress /home/aistudio/.paddlehub/dataset/chnsenticorp.tar.gz\n",
      "[==================================================] 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-06 01:54:21,040] [    INFO] - Installing lac module\n",
      "[2019-12-06 01:54:21,044] [    INFO] - Module lac already installed in /home/aistudio/.paddlehub/modules/lac\n",
      "[2019-12-06 01:54:23,939] [    INFO] - Checkpoint dir: nlp_senta_turtorial_demo\n"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "#coding:utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(name=\"senta_bilstm\")\n",
    "inputs, outputs, program = module.context(trainable=True)\n",
    "\n",
    "dataset = hub.dataset.ChnSentiCorp()\n",
    "reader = hub.reader.LACClassifyReader(\n",
    "    dataset=dataset, vocab_path=module.get_vocab_path())\n",
    "\n",
    "strategy = hub.AdamWeightDecayStrategy(\n",
    "    weight_decay=0.01,\n",
    "    warmup_proportion=0.1,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler=\"linear_decay\",\n",
    "    optimizer_name=\"adam\")\n",
    "\n",
    "config = hub.RunConfig(\n",
    "    use_data_parallel=False,\n",
    "    use_pyreader=False,\n",
    "    use_cuda=True,\n",
    "    batch_size=1,\n",
    "    enable_memory_optim=False,\n",
    "    checkpoint_dir=\"nlp_senta_turtorial_demo\",\n",
    "    strategy=strategy)\n",
    "\n",
    "sent_feature = outputs[\"sentence_feature\"]\n",
    "\n",
    "feed_list = [inputs[\"words\"].name]\n",
    "\n",
    "cls_task = hub.TextClassifierTask(\n",
    "    data_reader=reader,\n",
    "    feature=sent_feature,\n",
    "    feed_list=feed_list,\n",
    "    num_classes=dataset.num_labels,\n",
    "    config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 筛选、分词、情感分析、入库\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pymysql\n",
    "import time\n",
    "import math\n",
    "\n",
    "TOTAL_DATA=100000\n",
    "PER_TIME=20\n",
    "SEARCH_TIME=math.ceil(TOTAL_DATA/PER_TIME)\n",
    "\n",
    "db = pymysql.connect(\"localhost\",\"root\",\"123456\",\"weibo\" )\n",
    "cursor = db.cursor()\n",
    "\n",
    "dataStore=[]\n",
    "sentimentData=[]\n",
    "\n",
    "month={'Jan':'1','Feb':'2','Mar':'3','Apr':'4','May':'5','Jun':'6','Jul':'7','Aug':'8','Sep':'9','Oct':'10','Nov':'11','Dec':'12'}\n",
    "'''\n",
    "def insertTONew(data):\n",
    "    insert_sql = \"\"\"insert into DataFiltered(mblogid,publish_at,content,username,pic_count,gender,followers_count,friends_count,statuses_count,user_created_at,place,created_at,hour,keyword,sentiment)\\\n",
    "         value ('{0[1]}','{0[2]}','{0[3]}','{0[4]}','{0[5]}','{0[6]}','{0[7]}','{0[8]}','{0[9]}','{0[10]}','{0[11]}','{0[12]}','{0[13]}','{0[14]}','{0[15]}'),\\\n",
    "            ('{1[1]}','{1[2]}','{1[3]}','{1[4]}','{1[5]}','{1[6]}','{1[7]}','{1[8]}','{1[9]}','{1[10]}','{1[11]}','{1[12]}','{1[13]}','{1[14]}','{1[15]}'),\\\n",
    "            ('{2[1]}','{2[2]}','{2[3]}','{2[4]}','{2[5]}','{2[6]}','{2[7]}','{2[8]}','{2[9]}','{2[10]}','{2[11]}','{2[12]}','{2[13]}','{2[14]}','{2[15]}'),\\\n",
    "            ('{3[1]}','{3[2]}','{3[3]}','{3[4]}','{3[5]}','{3[6]}','{3[7]}','{3[8]}','{3[9]}','{3[10]}','{3[11]}','{3[12]}','{3[13]}','{3[14]}','{3[15]}'),\\\n",
    "            ('{4[1]}','{4[2]}','{4[3]}','{4[4]}','{4[5]}','{4[6]}','{4[7]}','{4[8]}','{4[9]}','{4[10]}','{4[11]}','{4[12]}','{4[13]}','{4[14]}','{4[15]}'),\\\n",
    "            ('{5[1]}','{5[2]}','{5[3]}','{5[4]}','{5[5]}','{5[6]}','{5[7]}','{5[8]}','{5[9]}','{5[10]}','{5[11]}','{5[12]}','{5[13]}','{5[14]}','{5[15]}'),\\\n",
    "            ('{6[1]}','{6[2]}','{6[3]}','{6[4]}','{6[5]}','{6[6]}','{6[7]}','{6[8]}','{6[9]}','{6[10]}','{6[11]}','{6[12]}','{6[13]}','{6[14]}','{6[15]}'),\\\n",
    "            ('{7[1]}','{7[2]}','{7[3]}','{7[4]}','{7[5]}','{7[6]}','{7[7]}','{7[8]}','{7[9]}','{7[10]}','{7[11]}','{7[12]}','{7[13]}','{7[14]}','{7[15]}'),\\\n",
    "            ('{8[1]}','{8[2]}','{8[3]}','{8[4]}','{8[5]}','{8[6]}','{8[7]}','{8[8]}','{8[9]}','{8[10]}','{8[11]}','{8[12]}','{8[13]}','{8[14]}','{8[15]}'),\\\n",
    "            ('{9[1]}','{9[2]}','{9[3]}','{9[4]}','{9[5]}','{9[6]}','{9[7]}','{9[8]}','{9[9]}','{9[10]}','{9[11]}','{9[12]}','{9[13]}','{9[14]}','{9[15]}'),\\\n",
    "            ('{10[1]}','{10[2]}','{10[3]}','{10[4]}','{10[5]}','{10[6]}','{10[7]}','{10[8]}','{10[9]}','{10[10]}','{10[11]}','{10[12]}','{10[13]}','{10[14]}','{10[15]}'),\\\n",
    "            ('{11[1]}','{11[2]}','{11[3]}','{11[4]}','{11[5]}','{11[6]}','{11[7]}','{11[8]}','{11[9]}','{11[10]}','{11[11]}','{11[12]}','{11[13]}','{11[14]}','{11[15]}'),\\\n",
    "            ('{12[1]}','{12[2]}','{12[3]}','{12[4]}','{12[5]}','{12[6]}','{12[7]}','{12[8]}','{12[9]}','{12[10]}','{12[11]}','{12[12]}','{12[13]}','{12[14]}','{12[15]}'),\\\n",
    "            ('{13[1]}','{13[2]}','{13[3]}','{13[4]}','{13[5]}','{13[6]}','{13[7]}','{13[8]}','{13[9]}','{13[10]}','{13[11]}','{13[12]}','{13[13]}','{13[14]}','{13[15]}'),\\\n",
    "            ('{14[1]}','{14[2]}','{14[3]}','{14[4]}','{14[5]}','{14[6]}','{14[7]}','{14[8]}','{14[9]}','{14[10]}','{14[11]}','{14[12]}','{14[13]}','{14[14]}','{14[15]}'),\\\n",
    "            ('{15[1]}','{15[2]}','{15[3]}','{15[4]}','{15[5]}','{15[6]}','{15[7]}','{15[8]}','{15[9]}','{15[10]}','{15[11]}','{15[12]}','{15[13]}','{15[14]}','{15[15]}'),\\\n",
    "            ('{16[1]}','{16[2]}','{16[3]}','{16[4]}','{16[5]}','{16[6]}','{16[7]}','{16[8]}','{16[9]}','{16[10]}','{16[11]}','{16[12]}','{16[13]}','{16[14]}','{16[15]}'),\\\n",
    "            ('{17[1]}','{17[2]}','{17[3]}','{17[4]}','{17[5]}','{17[6]}','{17[7]}','{17[8]}','{17[9]}','{17[10]}','{17[11]}','{17[12]}','{17[13]}','{17[14]}','{17[15]}'),\\\n",
    "            ('{18[1]}','{18[2]}','{18[3]}','{18[4]}','{18[5]}','{18[6]}','{18[7]}','{18[8]}','{18[9]}','{18[10]}','{18[11]}','{18[12]}','{18[13]}','{18[14]}','{18[15]}'),\\\n",
    "            ('{19[1]}','{19[2]}','{19[3]}','{19[4]}','{19[5]}','{19[6]}','{19[7]}','{19[8]}','{19[9]}','{19[10]}','{19[11]}','{19[12]}','{19[13]}','{19[14]}','{19[15]}')\n",
    "            \"\"\".format(data[0],data[1],data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10],data[11],data[12],data[13],data[14],data[15],data[16],data[17],data[18],data[19])\n",
    "    cursor.execute(insert_sql)\n",
    "    db.commit()\n",
    "'''\n",
    "def insertOneToNew(data):\n",
    "    insert_sql = \"\"\"insert into DataFiltered(mblogid,publish_at,content,username,pic_count,gender,followers_count,friends_count,statuses_count,user_created_at,place,created_at,hour,keyword,sentiment)\\\n",
    "         value ('{0[1]}','{0[2]}','{0[3]}','{0[4]}','{0[5]}','{0[6]}','{0[7]}','{0[8]}','{0[9]}','{0[10]}','{0[11]}','{0[12]}','{0[13]}','{0[14]}',{0[15]})\"\"\".format(data)\n",
    "    cursor.execute(insert_sql)\n",
    "    db.commit()\n",
    "\n",
    "def getTimeStamp(timeString):\n",
    "        timeformated=timeString[5]+'-'+month[timeString[1]]+'-'+timeString[2]+' '+timeString[3]\n",
    "        timeArray = time.strptime(timeformated, \"%Y-%m-%d %H:%M:%S\")\n",
    "        timeStamp = int(time.mktime(timeArray))\n",
    "        return timeStamp\n",
    "        \n",
    "def domain(se_sql):\n",
    "    global dataStore\n",
    "    global sentimentData\n",
    "    global PER_TIME\n",
    "    f=open('./work/test.txt','a')\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(se_sql)\n",
    "        results=cursor.fetchall()\n",
    "        for row in results:\n",
    "            date=row[2].split(' ')\n",
    "            timehour=date[3].split(':')\n",
    "            if int(timehour[0])<6:\n",
    "                timeSp=getTimeStamp(row[2].split(' '))\n",
    "                insertData=list(row)\n",
    "                insertData[2]=timeSp\n",
    "                insertData[3]=insertData[3].replace(\"\\n\", \" \").replace(\"\\\"\",\" \")\n",
    "                keyword=' '.join(jieba.analyse.extract_tags(insertData[3]))  #分词\n",
    "                f.write(keyword+'\\n')\n",
    "                insertData.append(timehour[0])\n",
    "                insertData.append(keyword)\n",
    "                \n",
    "                run_states = cls_task.predict(data=[insertData[3]])   #模型预测\n",
    "                results = [run_state.run_results for run_state in run_states]\n",
    "                # TODO 修改\n",
    "                if len(results)>0:\n",
    "                    res=res=np.argmax(results[0], axis=2)[0]\n",
    "                    if len(res)>0:\n",
    "                        insertData.append(res[0])\n",
    "                    else:\n",
    "                        insertData.append(3)\n",
    "                else:\n",
    "                    insertData.append(4)\n",
    "                # dataStore.append(insertData)\n",
    "                # sentimentData.append(insertData[3])\n",
    "                insertOneToNew(insertData)\n",
    "                # insertTONew(insertData)\n",
    "            # if len(dataStore)==PER_TIME:\n",
    "            #     print(\"daol-------\"+str(len(sentimentData)))\n",
    "            #     run_states = cls_task.predict(data=sentimentData)\n",
    "            #     results = [run_state.run_results for run_state in run_states]\n",
    "            #     for index,OneData in enumerate(dataStore):\n",
    "            #         res=np.argmax(results[index], axis=2)[0]\n",
    "            #         OneData.append(res[0])\n",
    "            #     insertTONew(dataStore)\n",
    "            #     sentimentData=[]\n",
    "            #     dataStore=[]        \n",
    "    except Exception as e:\n",
    "        print(\"err happen\"+str(e))\n",
    "    f.close()\n",
    "\n",
    "#每次取{PER_TIME}条数据    \n",
    "i=0\n",
    "bid=0\n",
    "while i<SEARCH_TIME:\n",
    "    if i==0:\n",
    "        sql=\"SELECT id from DataUnion limit 1\"\n",
    "        cursor.execute(sql)\n",
    "        res=cursor.fetchone()\n",
    "        bid=res[0]\n",
    "    else:\n",
    "        bid=PER_TIME+bid\n",
    "    if i%10==0:\n",
    "        print(\"------正在进行第%d轮,起始ID为%d------\"%(i,bid))\n",
    "    s_sql=\"SELECT * FROM DataUnion where id>=%d limit %d\"%(bid,PER_TIME)\n",
    "    domain(s_sql)\n",
    "    i+=1\n",
    "\n",
    "# for oneData in dataStore:\n",
    "#     insertOneToNew(oneData)\n",
    "\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7fabef484850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 绘制词云\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from imageio import imread\n",
    "\n",
    "\n",
    "mk=imread(\"./work/background.jpg\")\n",
    "f=open(u'./work/test.txt','r').read()\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "word=WordCloud(font_path='./work/kaitiGB2312.ttf',mask=mk,background_color='white',scale=16,collocations=False, width=1280,height=960,margin=2,stopwords=stopwords).generate(f)\n",
    "\n",
    "word.to_file('./work/word2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.6.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
